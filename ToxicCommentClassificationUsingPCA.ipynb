{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprosessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from scipy.special import logit, expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv').fillna(' ')\n",
    "test = pd.read_csv('test.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train.sort_values(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Explanation\\nWhy the edits made under my usern...\n",
       "1         D'aww! He matches this background colour I'm s...\n",
       "2         Hey man, I'm really not trying to edit war. It...\n",
       "3         \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4         You, sir, are my hero. Any chance you remember...\n",
       "5         \"\\n\\nCongratulations from me as well, use the ...\n",
       "6              COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
       "7         Your vandalism to the Matt Shirvington article...\n",
       "8         Sorry if the word 'nonsense' was offensive to ...\n",
       "9         alignment on this subject and which are contra...\n",
       "10        \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...\n",
       "11        bbq \\n\\nbe a man and lets discuss it-maybe ove...\n",
       "12        Hey... what is it..\\n@ | talk .\\nWhat is it......\n",
       "13        Before you start throwing accusations and warn...\n",
       "14        Oh, and the girl above started her arguments w...\n",
       "15        \"\\n\\nJuelz Santanas Age\\n\\nIn 2002, Juelz Sant...\n",
       "16        Bye! \\n\\nDon't look, come or think of comming ...\n",
       "17         REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski\n",
       "18        The Mitsurugi point made no sense - why not ar...\n",
       "19        Don't mean to bother you \\n\\nI see that you're...\n",
       "20        \"\\n\\n Regarding your recent edits \\n\\nOnce aga...\n",
       "21        \"\\nGood to know. About me, yeah, I'm studying ...\n",
       "22        \"\\n\\n Snowflakes are NOT always symmetrical! \\...\n",
       "23        \"\\n\\n The Signpost: 24 September 2012 \\n\\n Rea...\n",
       "24        \"\\n\\nRe-considering 1st paragraph edit?\\nI don...\n",
       "25        Radial symmetry \\n\\nSeveral now extinct lineag...\n",
       "26        There's no need to apologize. A Wikipedia arti...\n",
       "27        Yes, because the mother of the child in the ca...\n",
       "28        \"\\nOk. But it will take a bit of work but I ca...\n",
       "29        \"== A barnstar for you! ==\\n\\n  The Real Life ...\n",
       "                                ...                        \n",
       "153134    \" \\n\\n == Same coffee shop? == \\n\\n My memory ...\n",
       "153135    SO many things wrong with that viewpoint - fro...\n",
       "153136    \" \\n\\n Unless we have an article for some othe...\n",
       "153137    Hannah and Maddie are soooooo awesome and are ...\n",
       "153138    :::no problem, I tagged it and cleaned it out....\n",
       "153139    :PS I've just looked at the history of this ar...\n",
       "153140    \"== Your edit to Maungaturoto == \\n Please don...\n",
       "153141    :If you wish to contest the prod, please remov...\n",
       "153142    Balancing the two approaches to psychiatry ( b...\n",
       "153143                                   Ah, suck my balls.\n",
       "153144    == Your name mentioned == \\n Hi, I just though...\n",
       "153145    I've just discovered yet another list: List of...\n",
       "153146    ==Wikiproject Video Games assessment== \\n I do...\n",
       "153147    ::Consensus for ruining Wikipedia? I think tha...\n",
       "153148    == DAP ?  == \\n\\n What's point with DAP ?! Naz...\n",
       "153149    shut down the mexican border withought looking...\n",
       "153150    :Jerome, I see you never got around to thisâ€¦! ...\n",
       "153151    ==Lucky bastard== \\n http://wikimediafoundatio...\n",
       "153152    ==WTF== \\n It's no longer a redlink.  Now what...\n",
       "153153    \" \\n\\n ==\"\"Illness\"\" no shit== \\n Just for the...\n",
       "153154    ==shame on you all!!!== \\n\\n You want to speak...\n",
       "153155    MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...\n",
       "153156    \" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...\n",
       "153157    :Disagree. Soviet railways need their own arti...\n",
       "153158    This idiot can't even use proper grammar when ...\n",
       "153159    . \\n i totally agree, this stuff is nothing bu...\n",
       "153160    == Throw from out field to home plate. == \\n\\n...\n",
       "153161    \" \\n\\n == Okinotorishima categories == \\n\\n I ...\n",
       "153162    \" \\n\\n == \"\"One of the founding nations of the...\n",
       "153163    \" \\n :::Stop already. Your bullshit is not wel...\n",
       "Name: comment_text, Length: 312735, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])\n",
    "all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=10000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (charcter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 5),\n",
    "    max_features=25000)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10000)\n",
    "train_car_features = pca.fit_transform(train_char_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#losses = []\n",
    "predictions = {'id': test['id']}\n",
    "for class_name in class_names:\n",
    "    print(class_name)\n",
    "    train_target = train[class_name]\n",
    "    classifier = LogisticRegression(solver='sag')\n",
    "\n",
    "#     cv_loss = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "#     losses.append(cv_loss)\n",
    "#     print('CV score for class {} is {}'.format(class_name, cv_loss))\n",
    "\n",
    "\n",
    "    classifier.fit(train_features, train_target)\n",
    "    predictions[class_name] = classifier.predict_proba(test_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print('Total CV score is {}'.format(np.mean(losses)))\n",
    "submission = pd.DataFrame.from_dict(predictions)\n",
    "class_names.insert(0, 'id')\n",
    "submission = submission.loc[:,class_names]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
